---
title: "Class 8: Unsupervised Learning Mini-Project"
author: "Kris Price (PID: A17464127)"
format: pdf
toc: true
---

## Background

### Preparing the Data

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
```

```{r}
head(wisc.df)
```

Make sure to remove the first `diagnosis` column - I don't want to use this for my machine learning models. We will use it later to compare our results to the expert diagnosis.

```{r}
wisc.data <- wisc.df[, -1]
diagnosis <- wisc.df$diagnosis
```

### Exploratory Data Analysis

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.df)
```

There are 569 observations in this dataset.

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

212 of these observations have a malignant diagnosis.

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean", colnames(wisc.df)))
```

10 of the variables are suffixed with _mean.

## Principal Component Analysis

### Performing PCA

```{r}
colMeans(wisc.data)
apply(wisc.data, 2, sd)
```

```{r}
wisc.pr <- prcomp(wisc.data, scale = T)

summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

PC1 captures 44.27% of the original variance.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 principle components.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 principle components.

### Interpreting PCA Results

Our main PCA "score plot" or "PC plot" of results:

```{r}
library(ggplot2)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```

This plot is very difficult to understand because it plots each observation as its rowname text, and all the text overlaps with each other.

```{r}
ggplot(wisc.pr$x, aes(PC1, PC2, col = diagnosis)) +
  geom_point()
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(wisc.pr$x, aes(PC1, PC3, col = diagnosis)) +
  geom_point()
```

In both of these plots, the Malignant and Benign patients are separated into their own clusters. However, the clusters in the PC1 vs. PC3 plot aren't as strongly separated as the clusters in the PC1 vs. PC2 plot.

### Variance Explained

```{r}
pr.var <- (wisc.pr$sdev)^2
head(pr.var)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

The component of the loading vector for `concave.points_mean` is -0.261. No other features have a larger contribution than this one.

## Hierarchical Clustering

```{r}
data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist)

plot(wisc.hclust)
abline(h = 19, col = "red", lty = 2)
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The clustering model has 4 clusters at height = 19.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

table(wisc.hclust.clusters, diagnosis)
```

> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I like `method = "ward.D2"` the most because it produces the dendogram that's the easiest to read.

## Combining Methods

Here we will take our PCA results and use those as input for clustering. In other words, our `wisc.pr$x` scores that we plotted above (the main output from PCA - how the data lie on our new principle component axis) and use a subset of the PCs as input for `hclust()`

```{r}
pc.dist <- dist(wisc.pr$x[, 1:3])
wisc.pr.hclust <- hclust(pc.dist, method = "ward.D2")
plot(wisc.pr.hclust)
```

Cut the dendogram/tree into two main groups/clusters:

```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```

I want to know how the clustering in `grps` with values of 1 or 2 correspond to the expert `diagnosis`

```{r}
table(grps, diagnosis)
```

My clustering **group 1** are mostly "M" diagnoses (179) and my clustering **group 2** are mostly "B" diagnoses (333)

24 FP (false positives)
179 TP (true positives)
333 TN (true negatives)
33 FN (false negatives)

## Prediction

```{r}
new <- read.csv("new_samples.csv")
npc <- predict(wisc.pr, newdata = new)
npc
```

```{r}
plot(wisc.pr$x[, 1:2], col = grps)
points(npc[, 1], npc[, 2], col = "blue", pch = 16, cex = 3)
text(npc[, 1], npc[, 2], c(1,2), col = "white")
```

> Q16. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize following up with the new patients in cluster 2.
